{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93795c68",
   "metadata": {},
   "source": [
    "# Discretized 1D Kohn-Sham Equation\n",
    "\n",
    "## Problem Description\n",
    "In this part, we consider the singleparticle Hamiltonian arising from discretizing an 1D Kohn-Sham equation in electronic structure\n",
    "calculations,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min_{X \\in \\mathbb{R}^{n\\times p}}\\quad &\\frac{1}{2} \\mathrm{tr}\\left( X^\\top LX \\right) + \\frac{\\alpha}{4} \\rho^\\top L^{-1} \\rho\\\\\n",
    "    \\text{s. t.} \\quad &X^\\top X = I_p,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\rho := \\mathrm{Diag}(XX^\\top)$, $L$ is a tri-diagonal matrix with $2$ on its diagonal and $-1$ on its subdiagonal, and $\\alpha > 0$ is a parameter. Such problems have become standard testing problems for investigating the convergence of self-consistent field methods due to its simplicity. Clearly, these problems are smooth optimization problems on the Stiefel manifold, and we show how to solve these problems with `cdopt` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7d7d3",
   "metadata": {},
   "source": [
    "## Importing modules\n",
    "We first import all the necessary modules for this optimization problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666297ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing backbone_jax Possibly JAX is not installed.\n",
      "No JAX package installed\n"
     ]
    }
   ],
   "source": [
    "import cdopt \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afd96d",
   "metadata": {},
   "source": [
    "## Generating datas\n",
    "We generate necessary data. Notice that $L$ is a sparse matrix, we apply the functions from `scipy.sparse` to accelerate the computation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd0b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "p = 10\n",
    "alpha = 1\n",
    "L = diags([-1,2,-1], [-1,0,1], format='csr', shape=(n,n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94453b",
   "metadata": {},
   "source": [
    "## Set functions and problems\n",
    "\n",
    "Then we set the objective function and the Stiefel manifold. \n",
    "Notice that all the existing AD packages has limited support for operations on sparse matrices, we manually define the gradient and Hessian-vector product of the objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474fe6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fun(X):\n",
    "    rho = np.sum(X ** 2, 1)\n",
    "    Linvrho = spsolve(L, rho)\n",
    "    LX = L @ X\n",
    "    return 0.5 * np.sum(X*LX) + alpha/4 * np.sum(Linvrho * rho)\n",
    "\n",
    "def obj_grad(X):\n",
    "    rho = np.sum(X ** 2, 1)\n",
    "    Linvrho = alpha * spsolve(L, rho)\n",
    "    return L@ X + Linvrho[:, np.newaxis] * X \n",
    "\n",
    "def obj_hvp(X, D): \n",
    "    rho = np.sum(X ** 2, 1)\n",
    "    rhoXdot = 2*np.sum(X*D, 1)\n",
    "    LinvrhoXdot = alpha * spsolve(L, rhoXdot)\n",
    "    Linvrho = alpha * spsolve(L, rho)\n",
    "    return L @ D + LinvrhoXdot[:, np.newaxis] * X  + Linvrho[:, np.newaxis] * D \n",
    "\n",
    "M = cdopt.manifold_np.stiefel_np((n,p))   # The Stiefel manifold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bffa6",
   "metadata": {},
   "source": [
    "## Describe the optimization problem \n",
    "\n",
    "The optimization problem can be described by the manifold and the drivatives of objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea0aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_test = cdopt.core.Problem(M, obj_fun,obj_grad=obj_grad, obj_hvp=obj_hvp, beta = 30)  # describe the optimization problem and set the penalty parameter \\beta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8531a",
   "metadata": {},
   "source": [
    "## Apply optimization problem\n",
    "\n",
    "After describe the optimization problem, we can directly function value, gradient and Hessian-vector product from the `cdopt.core.Problem` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39789d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver   fval         iter   f_eval   stationarity   feaibility     CPU time\n",
      "& L-BFGS & 3.57e+01  & 173  & 180    & 4.28e-06     & 3.30e-08     & 0.32 \\\\\n"
     ]
    }
   ],
   "source": [
    "# the vectorized function value, gradient and Hessian-vector product of the constraint dissolving function. Their inputs are numpy 1D array, and their outputs are float or numpy 1D array.\n",
    "cdf_fun_np = problem_test.cdf_fun_vec_np   \n",
    "cdf_grad_np = problem_test.cdf_grad_vec_np \n",
    "cdf_hvp_np = problem_test.cdf_hvp_vec_np\n",
    "\n",
    "\n",
    "## Apply limit memory BFGS solver from scipy.minimize \n",
    "from scipy.optimize import fmin_bfgs, fmin_cg, fmin_l_bfgs_b, fmin_ncg\n",
    "Xinit = M.tensor2array(M.Init_point())  # set initial point\n",
    "\n",
    "t_start = time.time()\n",
    "out_msg = sp.optimize.minimize(cdf_fun_np, Xinit.flatten(),method='L-BFGS-B',jac = cdf_grad_np, options={'disp': None, 'maxcor': 10, 'ftol': 0, 'gtol': 1e-06, 'eps': 0e-08,})\n",
    "t_end = time.time() - t_start\n",
    "\n",
    "# Statistics\n",
    "feas = M.Feas_eval(M.v2m(M.array2tensor(out_msg.x)))   # Feasibility\n",
    "stationarity = np.linalg.norm(out_msg['jac'],2)   # stationarity\n",
    "\n",
    "result_lbfgs = [out_msg['fun'], out_msg['nit'], out_msg['nfev'],stationarity,feas, t_end]\n",
    "\n",
    "# print results\n",
    "print('Solver   fval         iter   f_eval   stationarity   feaibility     CPU time')\n",
    "print('& L-BFGS & {:.2e}  & {:}  & {:}    & {:.2e}     & {:.2e}     & {:.2f} \\\\\\\\'.format(*result_lbfgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002ebf2",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1.  Lin L, Yang C. Elliptic preconditioner for accelerating the self-consistent field iteration in Kohn--Sham density functional theory[J]. SIAM Journal on Scientific Computing, 2013, 35(5): S277-S298.\n",
    "2.  Xiao N, Liu X. Solving Optimization Problems over the Stiefel Manifold by Smooth Exact Penalty Function[J]. arXiv preprint arXiv:2110.08986, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2976304",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07d5d4ebd289d7e3a8d5104fbc288ece76787e92d17b572773bddf91a0286b7c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cdopt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
