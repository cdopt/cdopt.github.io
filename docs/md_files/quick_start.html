
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tutorial &#8212; An Introduction to CDF</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="API Reference" href="api_reference.html" />
    <link rel="prev" title="Overview" href="overview.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">An Introduction to CDF</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to CDOpt
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="overview.html">
   Overview
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Tutorial
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="api_reference.html">
   API Reference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="apis/cdopt_core.html">
     cdopt.core
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="apis/cdopt_manifold.html">
     cdopt.manifold
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/md_files/quick_start.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xnchxy/constraint_dissolving_lib"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xnchxy/constraint_dissolving_lib/issues/new?title=Issue%20on%20page%20%2Fmd_files/quick_start.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#installation">
   Installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quickstart-a-simple-example">
   Quickstart: a simple example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#manifolds">
   Manifolds
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solvers">
   Solvers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#automatic-differentiation-backbones">
   Automatic differentiation backbones
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cuda-support">
   CUDA support
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Tutorial</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#installation">
   Installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quickstart-a-simple-example">
   Quickstart: a simple example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#manifolds">
   Manifolds
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#solvers">
   Solvers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#automatic-differentiation-backbones">
   Automatic differentiation backbones
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cuda-support">
   CUDA support
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">cdopt</span></code>  is an easy-to-use modular package that separates the manifold modules, the problems descriptions, the automatic differentiation packages and solvers apart. All of the automatic differentiations are done behind the scenes so the amount of setup that user needs to do is minimal. Usually only the following steps are required:</p>
<ol class="simple">
<li><p>Instantiate a manifold <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> from the <code class="docutils literal notranslate"><span class="pre">cdopt.manifold</span></code> or define <span class="math notranslate nohighlight">\(\mathcal{M} = \{x \in \mathbb{R}^n: c(x) = 0_p \}\)</span> by the <code class="docutils literal notranslate"><span class="pre">cdopt.manifold.basic_manifold</span></code> module.</p></li>
<li><p>Define a cost function <span class="math notranslate nohighlight">\(f: \mathbb{R}^{m\times s} \to \mathbb{R}\)</span> to minimize over the manifold <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>.</p></li>
<li><p>Using the <code class="docutils literal notranslate"><span class="pre">cdopt.core.problem</span></code> as a high-level interface to describe the optimization problem.</p></li>
<li><p>Retrieve the corresponding constraint dissolving function and its differentials. Instantiate a solver from various of existing packages, including <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.optimizers</span></code>, to minimize the constraint dissolving function without any constraints.</p></li>
</ol>
<p>It is worth mentioning that <code class="docutils literal notranslate"><span class="pre">cdopt.problem</span></code> integrates various pre-processing and concurrency checking steps for the optimization problems. Moreover, it provides an integrated API for calling the related solvers. Therefore, although we can run the solvers without the <code class="docutils literal notranslate"><span class="pre">cdopt.core.problem</span></code> interface, using  <code class="docutils literal notranslate"><span class="pre">cdopt.core.problem</span></code> to define the problem is always recommended.</p>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">cdopt</span></code> is compatible with Python 3.6+, and depends on NumPy, SciPy and autograd.  To get the latest version of <code class="docutils literal notranslate"><span class="pre">cdopt</span></code>, you can install it via <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip install cdopt
</pre></div>
</div>
<p>Moreover, <code class="docutils literal notranslate"><span class="pre">cdopt</span></code> supports PyTorch&gt;=1.9.0 and JAX packages in its numerical computation. Therefore, we strongly suggest the users to install these packages.</p>
</div>
<div class="section" id="quickstart-a-simple-example">
<h2>Quickstart: a simple example<a class="headerlink" href="#quickstart-a-simple-example" title="Permalink to this headline">¶</a></h2>
<p>We begin with a simple optimization problem with orthogonality constraints,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
		\min_{X \in \mathbb{R}^{m\times s}}\quad &amp;f(X) = -\frac{1}{2}\mathrm{tr}(X^\top HX)\\
		\text{s.t.} \quad &amp; X^\top X = I_s,
	\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(H \in \mathbb{R}^{m\times m}\)</span> is a symmetric matrix, and the gradient of <span class="math notranslate nohighlight">\(f\)</span> can be expressed as</p>
<div class="math notranslate nohighlight">
\[
\nabla f(X) = -HX.
\]</div>
<p>The constraints on the matrix <span class="math notranslate nohighlight">\(X\)</span> require that <span class="math notranslate nohighlight">\(X\)</span> is an orthogonal matrix, i.e., <span class="math notranslate nohighlight">\(X\)</span> lies on the Stiefel manifold,</p>
<div class="math notranslate nohighlight">
\[
	\mathcal{S}_{m,s} = \{X \in \mathbb{R}^{m\times s}: X^\top X = I_s  \}. 
\]</div>
<p>The following is a minimal working example of how to solve the above problem using <code class="docutils literal notranslate"><span class="pre">cdopt</span></code> for a random symmetric matrix. As indicated in the introduction above, we follow four simple steps: we instantiate the manifold, create the cost function (using Autograd in this case), define a problem instance which we pass the manifold and the cost function, and run the minimization problem using one of the existing unconstrained optimization solvers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import basic functions</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin_bfgs</span><span class="p">,</span> <span class="n">fmin_cg</span><span class="p">,</span> <span class="n">fmin_l_bfgs_b</span><span class="p">,</span> <span class="n">fmin_ncg</span>
<span class="kn">import</span> <span class="nn">torch</span> 
<span class="kn">import</span> <span class="nn">cdopt</span>
<span class="kn">from</span> <span class="nn">cdopt.manifold_torch</span> <span class="kn">import</span> <span class="n">stiefel_torch</span>
<span class="kn">from</span> <span class="nn">cdopt.core.problem</span> <span class="kn">import</span> <span class="n">Problem</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># Set parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># column size</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">8</span>    <span class="c1"># row size</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># penalty parameter</span>
<span class="n">local_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>  <span class="c1"># the device to perform the computation</span>
<span class="n">local_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span>  <span class="c1"># the data type of the pytorch tensor</span>

<span class="c1"># Define object function</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span> <span class="o">=</span><span class="n">local_device</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">local_dtype</span><span class="p">)</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">H</span><span class="o">+</span><span class="n">H</span><span class="o">.</span><span class="n">T</span> 

<span class="k">def</span> <span class="nf">obj_fun</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">X</span> <span class="o">*</span> <span class="p">(</span><span class="n">H</span><span class="nd">@X</span><span class="p">))</span> 


<span class="c1"># Set optimization problems and retrieve constraint dissolving functions.</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">stiefel_torch</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">s</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span><span class="n">local_device</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">local_dtype</span> <span class="p">)</span>
<span class="n">problem_test</span> <span class="o">=</span> <span class="n">Problem</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">obj_fun</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span><span class="p">)</span>

<span class="n">cdf_fun_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_fun_vec_np</span>
<span class="n">cdf_grad_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_grad_vec_np</span>
<span class="n">cdf_hvp_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_hvp_vec_np</span>

<span class="c1"># Implement L-BFGS solver from scipy.optimize</span>
<span class="n">Xinit</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">tensor2array</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">Init_point</span><span class="p">())</span>
<span class="n">t_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">out_msg</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cdf_fun_np</span><span class="p">,</span> <span class="n">Xinit</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="n">cdf_grad_np</span><span class="p">)</span>
<span class="n">t_end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t_start</span>

<span class="c1"># Statistics</span>
<span class="n">feas</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">Feas_eval</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">v2m</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">array2tensor</span><span class="p">(</span><span class="n">out_msg</span><span class="o">.</span><span class="n">x</span><span class="p">)))</span>   <span class="c1"># Feasibility</span>
<span class="n">stationarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;jac&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span>   <span class="c1"># stationarity</span>
<span class="n">result_lbfgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;nit&#39;</span><span class="p">],</span> <span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;nfev&#39;</span><span class="p">],</span><span class="n">stationarity</span><span class="p">,</span><span class="n">feas</span><span class="p">,</span> <span class="n">t_end</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&amp; L-BFGS &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:}</span><span class="s1"> &amp; </span><span class="si">{:}</span><span class="s1"> &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:.2f}</span><span class="s1"> </span><span class="se">\\\\</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">result_lbfgs</span><span class="p">))</span>
</pre></div>
</div>
<p>Now let us take a deeper look at the code step by step. First, <code class="docutils literal notranslate"><span class="pre">cdopt</span></code> imports necessary packages and set the parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import basic functions</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin_bfgs</span><span class="p">,</span> <span class="n">fmin_cg</span><span class="p">,</span> <span class="n">fmin_l_bfgs_b</span><span class="p">,</span> <span class="n">fmin_ncg</span>
<span class="kn">import</span> <span class="nn">torch</span> 
<span class="kn">import</span> <span class="nn">cdopt</span>
<span class="kn">from</span> <span class="nn">cdopt.manifold_torch</span> <span class="kn">import</span> <span class="n">stiefel_torch</span>
<span class="kn">from</span> <span class="nn">cdopt.core.problem</span> <span class="kn">import</span> <span class="n">Problem</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># Set parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># column size</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">8</span>    <span class="c1"># row size</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># penalty parameter</span>
<span class="n">local_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>  <span class="c1"># the device to perform the computation</span>
<span class="n">local_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span>  <span class="c1"># the data type of the pytorch tensor</span>
</pre></div>
</div>
<p>Then we describe the objective function, where the variables are PyTorch tensors. The cost function should be a</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define object function</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span> <span class="o">=</span><span class="n">local_device</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">local_dtype</span><span class="p">)</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">H</span><span class="o">+</span><span class="n">H</span><span class="o">.</span><span class="n">T</span> 

<span class="k">def</span> <span class="nf">obj_fun</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">X</span> <span class="o">*</span> <span class="p">(</span><span class="n">H</span><span class="nd">@X</span><span class="p">))</span>  
</pre></div>
</div>
<p>Then we call <code class="docutils literal notranslate"><span class="pre">stiefel_torch</span></code> to generate a structure that describes the Stiefel manifold <span class="math notranslate nohighlight">\(\mathcal{S}_{n,p}\)</span>. This manifold corresponds to the constraint appearing in our optimization problem. For other constraints, take a look at the <a class="reference external" href="#manifolds">various supported manifolds</a> for details. The second instruction creates a structure named <code class="docutils literal notranslate"><span class="pre">problem_test</span></code>. Here the gradients and hessians of the objective function are not necessary, as they can be computed by the automatic differentiation (AD) packages. In our examples, we choose the  AD packages from PyTorch by setting <code class="docutils literal notranslate"><span class="pre">backbone</span> <span class="pre">=</span> <span class="pre">'torch'</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set optimization problems and retrieve constraint dissolving functions.</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">stiefel_torch</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">s</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span><span class="n">local_device</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">local_dtype</span> <span class="p">)</span>
<span class="n">problem_test</span> <span class="o">=</span> <span class="n">Problem</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">obj_fun</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
<p>After describe the optimization problem, we can directly retrieve the function value, gradients, Hessian of the corresponding constraint dissolving function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the objective function, gradient, hessian-vector product </span>
<span class="c1"># of the corresponding constraint dissolving function</span>
<span class="n">cdf_fun_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_fun_vec_np</span>   
<span class="n">cdf_grad_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_grad_vec_np</span>
<span class="n">cdf_hvp_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_hvp_vec_np</span>
</pre></div>
</div>
<p>Finally, we call the unconstraint solver from <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> package to minimize the constraint dissolving function over <span class="math notranslate nohighlight">\(\mathbb{R}^{n\times p}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implement L-BFGS solver from scipy.optimize</span>
<span class="n">Xinit</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">tensor2array</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">Init_point</span><span class="p">())</span>
<span class="n">t_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">out_msg</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cdf_fun_np</span><span class="p">,</span> <span class="n">Xinit</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="n">cdf_grad_np</span><span class="p">)</span>
<span class="n">t_end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t_start</span>

<span class="c1"># Statistics</span>
<span class="n">feas</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">Feas_eval</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">v2m</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">array2tensor</span><span class="p">(</span><span class="n">out_msg</span><span class="o">.</span><span class="n">x</span><span class="p">)))</span>   <span class="c1"># Feasibility</span>
<span class="n">stationarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;jac&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span>   <span class="c1"># stationarity</span>
<span class="n">result_lbfgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;nit&#39;</span><span class="p">],</span> <span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;nfev&#39;</span><span class="p">],</span><span class="n">stationarity</span><span class="p">,</span><span class="n">feas</span><span class="p">,</span> <span class="n">t_end</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&amp; L-BFGS &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:}</span><span class="s1"> &amp; </span><span class="si">{:}</span><span class="s1"> &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:.2f}</span><span class="s1"> </span><span class="se">\\\\</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">result_lbfgs</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="manifolds">
<h2>Manifolds<a class="headerlink" href="#manifolds" title="Permalink to this headline">¶</a></h2>
<p>For several well-known manifolds, we provide build-in expressions for <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> in the following table. We strongly suggest you to use the provided structures to define the manifold if it is included in the following table.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Expression of <span class="math notranslate nohighlight">\(c\)</span></p></th>
<th class="head"><p>Pre-defined structure from <code class="docutils literal notranslate"><span class="pre">autograd</span></code></p></th>
<th class="head"><p>Pre-defined structure from<code class="docutils literal notranslate"><span class="pre">PyTorch</span></code></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Euclidean space</p></td>
<td><p>No constraint</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.euclidean_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.euclidean_torch</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sphere</p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{ x \in \mathbb{R}^{n}: x^\top x = 1 \right\}\)</span></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.sphere_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.sphere_torch</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Oblique manifold</p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{ X \in \mathbb{R}^{m\times s}: \mathrm{Diag} (X ^\top X) = I_s \right\}\)</span></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.obluqie_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.obluqie_torch</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Stiefel manifold</p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{ X \in \mathbb{R}^{m\times s}: X ^\top X = I_s \right\}\)</span></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.stiefel_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.stiefel_torch</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Grassmann manifold</p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{ \mathrm{range}(X): X \in \mathbb{R}^{m\times s}, X ^\top X = I_s \right\}\)</span></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.stiefel_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.stiefel_torch</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Generalized Stiefel manifold</p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{ X \in \mathbb{R}^{m\times s}: X ^\top B X = I_s \right\}\)</span>, <span class="math notranslate nohighlight">\(B\)</span> is positive definite</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.generalized_stiefel_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.generalized_stiefel_torch</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Generalized Grassmann manifold</p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{ \mathrm{range}(X): X \in \mathbb{R}^{m\times s}, X ^\top B X = I_s \right\}\)</span>, <span class="math notranslate nohighlight">\(B\)</span> is positive definite</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.generalized_stiefel_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.generalized_stiefel_torch</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Hyperbolic manifold</p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{ X \in \mathbb{R}^{m\times s}: X ^\top B X = I_s \right\}\)</span>, <span class="math notranslate nohighlight">\(\lambda_{\min}(B)&lt; 0 &lt; \lambda_{\max}(B)\)</span></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.hyperbolic_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.hyperbolic_torch</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Symplectic Stiefel manifold</p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{ X \in \mathbb{R}^{2m\times 2s}: X ^\top Q_m X = Q_s \right\}\)</span>, <span class="math notranslate nohighlight">\(Q_m := \left[ \begin{smallmatrix}	{\bf 0}_{m\times m} &amp; I_m\\			 -I_m &amp; {\bf 0}_{m\times m}			\end{smallmatrix}\right]\)</span></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.symp_stiefel_np</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">manifold.symp_stiefel_torch</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>For generalized constraints <span class="math notranslate nohighlight">\(c(x) = 0\)</span>, the corresponding manifold can be directly constructed by the <code class="docutils literal notranslate"><span class="pre">cdopt.manifold.basic_manifold</span></code> structure, or its dependent class such as <code class="docutils literal notranslate"><span class="pre">cdopt.manifold.basic_manifold_np</span></code> and <code class="docutils literal notranslate"><span class="pre">cdopt.manifold.basic_manifold_torch</span></code>. one only needs to provide the expression of <span class="math notranslate nohighlight">\(c(x)\)</span>, then all the other essential materials can be automatically generated by the selected AD packages.</p>
<p>For example, consider the Riemannian manifold</p>
<div class="math notranslate nohighlight">
\[
\{ X \in \mathbb{R}^{m\times s}: X^\top XT = I_s \},
\]</div>
<p>where <span class="math notranslate nohighlight">\(T \in \mathbb{R}^{s\times s}\)</span> is a positive definite matrix (i.e., all of its eigenvalues is positive).  Although the compact formulation of such Riemannian manifold is not provided in <code class="docutils literal notranslate"><span class="pre">cdopt</span></code>, we can manually define it through <code class="docutils literal notranslate"><span class="pre">cdopt.manifold.basic_manifold_np</span></code>  and <code class="docutils literal notranslate"><span class="pre">autograd</span></code> package.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cdopt</span>
<span class="kn">from</span> <span class="nn">cdopt.manifold_np</span> <span class="kn">import</span> <span class="n">basic_manifold_np</span>
<span class="kn">from</span> <span class="nn">cdopt.core.problem</span> <span class="kn">import</span> <span class="n">Problem</span>
<span class="k">class</span> <span class="nc">my_manifold</span><span class="p">(</span><span class="n">basic_manifold_np</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Is</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s1">&#39;custom_manifold&#39;</span><span class="p">,(</span><span class="n">m</span><span class="p">,</span><span class="n">s</span><span class="p">),</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">s</span><span class="p">),</span>  <span class="n">regularize_value</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">C</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span><span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">T</span>  <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Is</span>
</pre></div>
</div>
<p>Then we can set the parameters <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(p\)</span> and randomly generate <span class="math notranslate nohighlight">\(T\)</span>, and initialize the Riemannian manifold in the following code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">s</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">T</span> <span class="o">@</span> <span class="n">T</span><span class="o">.</span><span class="n">T</span> 
<span class="n">T</span> <span class="o">=</span> <span class="n">T</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

<span class="n">M</span> <span class="o">=</span> <span class="n">my_manifold</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span> 
</pre></div>
</div>
<p>Next, we define the objective function. Notice that previously we set <code class="docutils literal notranslate"><span class="pre">backbone</span> <span class="pre">=</span> <span class="pre">'autograd'</span></code> in <code class="docutils literal notranslate"><span class="pre">my_manifold</span></code>. As a result, we need to construct the objective function that adapts to the <code class="docutils literal notranslate"><span class="pre">autograd</span></code> package, and set <code class="docutils literal notranslate"><span class="pre">backbone</span> <span class="pre">=</span> <span class="pre">'autograd'</span></code> in initializing the <code class="docutils literal notranslate"><span class="pre">Problem</span></code> structure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="nn">anp</span>
<span class="k">def</span> <span class="nf">obj_fun</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">anp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> 

<span class="n">problem_test</span> <span class="o">=</span> <span class="n">Problem</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">obj_fun</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">backbone</span> <span class="o">=</span> <span class="s1">&#39;autograd&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, we retrieve the gradients and hessians of CDF function from <code class="docutils literal notranslate"><span class="pre">problem.test</span></code> and apply <code class="docutils literal notranslate"><span class="pre">scipy.optimize.lbfgs</span></code> solver to minimize CDF.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cdf_fun_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_fun_vec_np</span>   
<span class="n">cdf_grad_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_grad_vec_np</span>
<span class="n">cdf_hvp_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_hvp_vec_np</span>


<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin_bfgs</span><span class="p">,</span> <span class="n">fmin_cg</span><span class="p">,</span> <span class="n">fmin_l_bfgs_b</span><span class="p">,</span> <span class="n">fmin_ncg</span>
<span class="c1"># Implement L-BFGS solver from scipy.optimize</span>
<span class="n">Xinit</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">tensor2array</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">Init_point</span><span class="p">())</span>
<span class="n">out_msg</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cdf_fun_np</span><span class="p">,</span> <span class="n">Xinit</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="n">cdf_grad_np</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="solvers">
<h2>Solvers<a class="headerlink" href="#solvers" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">cdopt</span></code> package does not contain any solvers. However, the unconstrained minimization of the constraint dissolving functions can be solved by various of existing solvers. The solvers from <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> and <code class="docutils literal notranslate"><span class="pre">torch_optimizer</span></code> can be directly applied to minimize CDF over <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>.</p>
</div>
<div class="section" id="automatic-differentiation-backbones">
<h2>Automatic differentiation backbones<a class="headerlink" href="#automatic-differentiation-backbones" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">cdopt</span></code> relies on the automatic differentiation (AD) packages to compute the derivatives of the objective function and build the constraint dissolving mapping. In <code class="docutils literal notranslate"><span class="pre">cdopt</span></code>, we provide various of plug-in AD backbones in <code class="docutils literal notranslate"><span class="pre">cdopt.core</span></code> based on <code class="docutils literal notranslate"><span class="pre">autograd</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code> packages. Moreover, one can easily build his own AD backbones by other packages, including the <code class="docutils literal notranslate"><span class="pre">jax</span></code> and <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>.</p>
</div>
<div class="section" id="cuda-support">
<h2>CUDA support<a class="headerlink" href="#cuda-support" title="Permalink to this headline">¶</a></h2>
<p>The CUDA support for<code class="docutils literal notranslate"><span class="pre">cdopt</span></code> relies on the employed backbones. Both the computation of constraint dissolving mappings and the unconstrained minimization of CDF can be accelerated by the CUDA support of the selected backbones.</p>
<p>For example, by setting the <code class="docutils literal notranslate"><span class="pre">local_device</span> <span class="pre">=</span> <span class="pre">torch.device('cuda')</span></code> in the following code blocks, all the computations for CDF are accelerated by CUDA.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import basic functions</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin_bfgs</span><span class="p">,</span> <span class="n">fmin_cg</span><span class="p">,</span> <span class="n">fmin_l_bfgs_b</span><span class="p">,</span> <span class="n">fmin_ncg</span>
<span class="kn">import</span> <span class="nn">torch</span> 
<span class="kn">import</span> <span class="nn">cdopt</span>
<span class="kn">from</span> <span class="nn">cdopt.manifold_torch</span> <span class="kn">import</span> <span class="n">stiefel_torch</span>
<span class="kn">from</span> <span class="nn">cdopt.core.problem</span> <span class="kn">import</span> <span class="n">Problem</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># Set parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># column size</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">8</span>    <span class="c1"># row size</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># penalty parameter</span>
<span class="n">local_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>  <span class="c1"># the device to perform the computation</span>
<span class="n">local_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span>  <span class="c1"># the data type of the pytorch tensor</span>

<span class="c1"># Define object function</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span> <span class="o">=</span><span class="n">local_device</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">local_dtype</span><span class="p">)</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">H</span><span class="o">+</span><span class="n">H</span><span class="o">.</span><span class="n">T</span> 

<span class="k">def</span> <span class="nf">obj_fun</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">X</span> <span class="o">*</span> <span class="p">(</span><span class="n">H</span><span class="nd">@X</span><span class="p">))</span> 


<span class="c1"># Set optimization problems and retrieve constraint dissolving functions.</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">stiefel_torch</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">s</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span><span class="n">local_device</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">local_dtype</span> <span class="p">)</span>
<span class="n">problem_test</span> <span class="o">=</span> <span class="n">Problem</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">obj_fun</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
<p>On the one hand, we can use the interface provided by <code class="docutils literal notranslate"><span class="pre">cdopt.core.Problem</span></code> class to retrieve the constraint dissolving function that adopts <code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> package.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cdf_fun_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_fun_vec_np</span>   
<span class="n">cdf_grad_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_grad_vec_np</span>
<span class="n">cdf_hvp_np</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_hvp_vec_np</span>

<span class="c1"># Implement L-BFGS solver from scipy.optimize</span>
<span class="n">Xinit</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">tensor2array</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">Init_point</span><span class="p">())</span>
<span class="n">out_msg</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cdf_fun_np</span><span class="p">,</span> <span class="n">Xinit</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="n">cdf_grad_np</span><span class="p">)</span>

<span class="c1"># Implement L-BFGS solver from scipy.optimize</span>
<span class="n">Xinit</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">tensor2array</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">Init_point</span><span class="p">())</span>
<span class="n">t_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">out_msg</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cdf_fun_np</span><span class="p">,</span> <span class="n">Xinit</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span><span class="n">jac</span> <span class="o">=</span> <span class="n">cdf_grad_np</span><span class="p">)</span>
<span class="n">t_end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t_start</span>

<span class="c1"># Statistics</span>
<span class="n">feas</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">Feas_eval</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">v2m</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">array2tensor</span><span class="p">(</span><span class="n">out_msg</span><span class="o">.</span><span class="n">x</span><span class="p">)))</span>   <span class="c1"># Feasibility</span>
<span class="n">stationarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;jac&#39;</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span>   <span class="c1"># stationarity</span>
<span class="n">result_lbfgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;fun&#39;</span><span class="p">],</span> <span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;nit&#39;</span><span class="p">],</span> <span class="n">out_msg</span><span class="p">[</span><span class="s1">&#39;nfev&#39;</span><span class="p">],</span><span class="n">stationarity</span><span class="p">,</span><span class="n">feas</span><span class="p">,</span> <span class="n">t_end</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&amp; L-BFGS &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:}</span><span class="s1"> &amp; </span><span class="si">{:}</span><span class="s1"> &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:.2e}</span><span class="s1"> &amp; </span><span class="si">{:.2f}</span><span class="s1"> </span><span class="se">\\\\</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">result_lbfgs</span><span class="p">))</span>
</pre></div>
</div>
<p>On the other hand, we can also employ the solvers from <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> and <code class="docutils literal notranslate"><span class="pre">torch_optimizer</span></code> packages to minimize the constraint dissolving function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cdf_fun</span> <span class="o">=</span> <span class="n">problem_test</span><span class="o">.</span><span class="n">cdf_fun</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xinit</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">Init_point</span><span class="p">(</span><span class="n">Xinit</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">cdf_fun</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">Init_point</span><span class="p">())</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
        <span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">for</span> <span class="n">jj</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">)):</span>
    <span class="n">train_epoch</span><span class="p">(</span><span class="n">jj</span><span class="p">)</span>
    
<span class="n">X_fin</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">X</span>
<span class="n">M</span><span class="o">.</span><span class="n">Feas_eval</span><span class="p">(</span><span class="n">X_fin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./md_files"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="overview.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Overview</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="api_reference.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">API Reference</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Nachuan Xiao, Xiaoyin Hu, Xin Liu, Kim-Chuan Toh<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>